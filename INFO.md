## Notes
* Will be using gpt-neo of some size

## TODO
* Read up on text generation using LLMs
* Gather more data (i.e. The Hobbit and other works) 
* Figure out how I want to format my Dataset class
  * 512 tokens? Paragraphs? Sentences?
  * Do I need to remove accented characters and special characters?
    * Or should I use this https://github.com/huggingface/tokenizers/issues/377

* Once data configuration is known, split data into train, val, test

## Useful links
* How to Fine-Tune GPT-2 For Text Generation
  * https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272

* Fine-tuning GPT-2 on Harry Potter texts for free
  * https://medium.com/@manjeetsingh_37967/fine-tuning-gpt-2-on-harry-potter-texts-for-free-718e9959aa89

* Fine Tuning GPT-2 for Magic the Gathering Flavour Text Generation
  * https://medium.com/swlh/fine-tuning-gpt-2-for-magic-the-gathering-flavour-text-generation-3bafd0f9bb93

* Fine-tuning GPT2 for Text Generation Using Pytorch
  * https://towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7


* Similar example
  * https://github.com/parvathysarat/gpt2-text-generation

